# V2 Ingestion Adapters — Multi-Source Signal Ingestion

> **Version:** 2.1 (Updated — per-adapter validation rules, input sanitization, encoding normalization)
> **Date:** 2026-02-09
> **Status:** Approved for Build

---

## 1. Universal Signal Format

All adapters produce signals in this normalized format before storage:

```typescript
interface RawSignal {
  id: string;                              // UUID, generated by normalizer
  source: SignalSource;
  source_id: string;                       // Original ID in source system
  source_url?: string;                     // Deep link back to source (if available)
  content: string;                         // Normalized text content
  content_type: ContentType;
  author?: string;                         // Who created the original content
  timestamp: Date;                         // Original content timestamp
  channel?: string;                        // Channel/context identifier
  metadata: Record<string, any>;           // Source-specific metadata
  attachments?: Attachment[];              // Referenced files/links
  parent_signal_id?: string;              // For threads/replies
  raw_content?: any;                       // Original untransformed content (for debug)
}

type SignalSource = 
  | 'slack' 
  | 'meeting_transcript' 
  | 'document' 
  | 'web_scrape' 
  | 'jira'          // TBD
  | 'wiki'          // TBD
  | 'community_forum'
  | 'grafana' 
  | 'splunk';

type ContentType = 
  | 'message' 
  | 'thread_reply'
  | 'transcript_segment'
  | 'document_chunk' 
  | 'ticket' 
  | 'wiki_page' 
  | 'web_page'
  | 'metric_alert' 
  | 'forum_post';

interface Attachment {
  type: 'file' | 'image' | 'link';
  name: string;
  url?: string;
  mime_type?: string;
}
```

---

## 2. Adapter Specifications

### 2.1 Slack Adapter (Existing — V1)

**Status:** Fully operational. No changes needed for V2 except routing through entity resolution.

**Mechanism:** MCP tool calls to Slack MCP server + direct @slack/web-api

**Files:**
- `backend/services/slack_llm_extractor.ts`
- `backend/services/slack_llm_extraction_service.ts`
- `backend/services/slack_structuring_service.ts`
- `scripts/ingest_raw_slack_to_signals.ts`

**V2 Enhancement:** After existing extraction, pipe entities through `entity_resolution_service` and sync to Neo4j.

---

### 2.2 Transcript Adapter (New)

**Mechanism:** Manual — PM pastes text or uploads file via MCP tool or REST API.

**Supported formats:** Plain text (paste), PDF, DOCX, VTT (WebVTT subtitles), SRT (SubRip subtitles), TXT

**API Endpoints:**

```
POST /api/ingest/transcript
  Content-Type: application/json (for text paste)
  Content-Type: multipart/form-data (for file upload)
```

**Processing Pipeline:**

```
1. Input arrives (text paste or file)
     ↓
2. If file: parse via Unstructured.io (Python document_parser service)
   If VTT/SRT: parse speaker turns and timestamps
   If text: use as-is
     ↓
3. Segment transcript into meaningful units:
   - If speaker turns available: one signal per speaker turn (group short turns)
   - If no speaker turns: semantic chunking (paragraph boundaries, ~500 tokens per chunk)
   - Minimum chunk: 50 tokens (skip "ok", "thanks", "let's move on")
     ↓
4. For each segment, create Signal:
   source = 'meeting_transcript'
   content_type = 'transcript_segment'
   metadata = {
     meeting_title: "...",
     meeting_date: "...",
     meeting_type: "sprint_review",
     speaker: "Alice" (if available),
     segment_index: 3,
     total_segments: 15,
     participants: ["Alice", "Bob"],
     customer: "Acme Corp" (if customer call)
   }
     ↓
5. Each signal → extraction pipeline → entity resolution → Neo4j sync
```

**Noise Filtering:**

Meeting transcripts are noisy. Filter out:
- Pure social/greeting segments ("Hi everyone", "Thanks for joining")
- Filler segments (<50 tokens with no nouns/entities)
- Repeated segments (transcript artifacts)

Keep:
- Action items and decisions
- Customer mentions and feedback quotes
- Feature discussions and requirements
- Issue reports and status updates
- Questions that reveal gaps or concerns

**LLM-assisted segment classification:**

```
For each transcript segment, classify:
{
  "segment_type": "action_item" | "decision" | "customer_feedback" | "feature_discussion" | 
                  "issue_report" | "status_update" | "question" | "social" | "filler",
  "relevance": 0.0-1.0,
  "entities_mentioned": [...],
  "summary": "Brief one-line summary"
}

Only create signals for segments with relevance > 0.3
Filtered segments (relevance ≤ 0.3) are stored as metadata on the parent
transcript document record for audit, but NOT created as individual signals.
This preserves the full transcript for reference while keeping the signal
pipeline clean.
```

**Edge case: No speaker turns available:**
```
If transcript has no speaker attribution:
  1. Use semantic chunking: split at paragraph boundaries
  2. Target: 300-800 tokens per chunk (minimum 50 tokens)
  3. Overlap: 50 tokens between chunks to preserve context at boundaries
  4. Each chunk gets metadata: { speaker: "unknown", segment_index: N }
```

**Edge case: Unstructured.io parsing failure:**
```
If document parser returns an error:
  1. Log error with file details to dead_letter_queue
  2. Return clear error to PM: "Failed to parse file. Supported formats: PDF, DOCX, VTT, SRT, TXT.
     Try converting to plain text and pasting directly."
  3. Offer text-paste fallback via ingest_transcript MCP tool
  4. DO NOT silently skip the file
```

---

### 2.3 Document Adapter (New)

**Mechanism:** Manual — PM uploads file via MCP tool or REST API.

**Supported formats:** PDF, DOCX, PPTX, XLSX, CSV, TXT

**API Endpoint:**

```
POST /api/ingest/document
  Content-Type: multipart/form-data
```

**Processing Pipeline:**

```
1. File uploaded to data/uploads/{timestamp}_{filename}
     ↓
2. Send to Python document_parser service (Unstructured.io)
   Returns: structured elements [
     { type: "Title", text: "Q4 Competitive Analysis", metadata: { page: 1 } },
     { type: "NarrativeText", text: "Competitor X launched...", metadata: { page: 1 } },
     { type: "Table", text: "Feature|Us|Comp A|...", metadata: { page: 3 } },
     { type: "ListItem", text: "Action: Review auth approach", metadata: { page: 5 } }
   ]
     ↓
3. Chunk elements by semantic boundaries:
   - Group contiguous elements from same section/page
   - Split at heading boundaries
   - Target: 300-800 tokens per chunk
   - Preserve table structure
     ↓
4. For each chunk, create Signal:
   source = 'document'
   content_type = 'document_chunk'
   metadata = {
     document_title: "Q4 Competitive Analysis",
     document_type: "competitive_analysis",
     file_name: "q4_comp_analysis.pptx",
     file_format: "pptx",
     page_number: 3,
     section_title: "Feature Comparison",
     chunk_index: 7,
     total_chunks: 22,
     element_types: ["Table", "NarrativeText"],
     author: "Product Team",
     customer: null,
     tags: ["competitive", "q4-2025"]
   }
     ↓
5. Each signal → extraction pipeline → entity resolution → Neo4j sync
```

**Format-specific handling:**

| Format | Unstructured.io Strategy | Notes |
|--------|--------------------------|-------|
| PDF | `partition_pdf` with OCR fallback | Handles scanned docs |
| DOCX | `partition_docx` | Preserves headings, lists, tables |
| PPTX | `partition_pptx` | Each slide as a section |
| XLSX | `partition_xlsx` | Each sheet separately; tables preserved |
| CSV | `partition_csv` | Column headers as context |
| TXT | `partition_text` | Paragraph-based chunking |

---

### 2.4 Crawler Bot Adapter (New)

**Mechanism:** External crawler bot POSTs scraped content to webhook endpoint.

**API Endpoint:**

```
POST /api/ingest/crawled
  Content-Type: application/json
```

**Expected Input:**

```typescript
interface CrawledContentInput {
  url: string;                          // Source URL
  title: string;                        // Page title
  content: string;                      // Cleaned text content (HTML stripped)
  scraped_at: string;                   // ISO 8601 timestamp
  content_type?: 'blog' | 'changelog' | 'pricing' | 'docs' | 'news' | 'forum' | 'other';
  competitor?: string;                  // Competitor name (if competitive intel)
  tags?: string[];                      // Tags for categorization
  metadata?: Record<string, any>;       // Any additional metadata from crawler
}
```

**Processing Pipeline:**

```
1. Crawled content arrives via POST
     ↓
2. Validate and normalize:
   - Ensure content is non-empty
   - Deduplicate by URL (skip if same URL scraped within 24h with same content hash)
   - Chunk long pages (>1000 tokens) into semantic sections
     ↓
3. For each chunk, create Signal:
   source = 'web_scrape'
   content_type = 'web_page'
   source_url = url
   metadata = {
     page_title: "...",
     content_type: "changelog",
     competitor: "Competitor X",
     scraped_at: "...",
     tags: [...]
   }
     ↓
4. Each signal → extraction pipeline → entity resolution → Neo4j sync
```

**Batch endpoint:**

```
POST /api/ingest/crawled/batch
  Body: { items: CrawledContentInput[] }
  Response: { ingested: 15, duplicates_skipped: 3, errors: 0 }
```

---

### 2.5 JIRA MCP Adapter (TBD Stub)

**Mechanism:** Will use JIRA MCP server when configured.

**Stub implementation:**

```typescript
// backend/adapters/jira_mcp_adapter.ts
export class JiraMcpAdapter {
  async ingest(): Promise<void> {
    throw new Error('JIRA MCP adapter not yet configured. Set JIRA_MCP_SERVER in .env');
  }
  
  isConfigured(): boolean {
    return !!process.env.JIRA_MCP_SERVER;
  }
}
```

**Manual fallback endpoint:**

```
POST /api/ingest/jira
  Body: { issues: JiraIssue[] }  // Manually exported JIRA issues as JSON
```

---

### 2.6 Wiki MCP Adapter (TBD Stub)

**Mechanism:** Will use Confluence/Wiki MCP server when configured.

**Stub implementation:**

```typescript
// backend/adapters/wiki_mcp_adapter.ts
export class WikiMcpAdapter {
  async ingest(): Promise<void> {
    throw new Error('Wiki MCP adapter not yet configured. Set WIKI_MCP_SERVER in .env');
  }
  
  isConfigured(): boolean {
    return !!process.env.WIKI_MCP_SERVER;
  }
}
```

**Manual fallback endpoint:**

```
POST /api/ingest/wiki
  Body: { pages: WikiPage[] }  // Manually exported wiki pages as JSON
```

---

## 3. Source Registry Service

Central registry tracking all connected data sources:

```typescript
interface SourceRegistration {
  id: string;
  source_type: SignalSource;
  name: string;                          // "Slack #customer-support", "Q4 Competitive Deck"
  adapter: string;                       // "slack_adapter", "transcript_adapter", etc.
  status: 'active' | 'configured' | 'stub' | 'error';
  config: Record<string, any>;           // Adapter-specific config
  last_sync_at?: Date;
  last_sync_status?: 'success' | 'partial' | 'error';
  signals_ingested: number;
  created_at: Date;
  updated_at: Date;
}
```

**API:**

```
GET  /api/sources                        -- List all registered sources
POST /api/sources                        -- Register new source
GET  /api/sources/:id                    -- Get source details + stats
PATCH /api/sources/:id                   -- Update source config
GET  /api/sources/:id/signals            -- List signals from this source
POST /api/sources/:id/sync              -- Trigger manual sync (for automated sources)
```

---

## 4. Normalizer Service

Validates, sanitizes, and enriches all incoming signals before storage.

See `08_DATA_CONTRACTS.md` §5 for the complete validation rule tables.

```typescript
class NormalizerService {
  normalize(input: AdapterOutput): RawSignal {
    // 1. Encoding validation (TV-1, TV-2, TV-3)
    this.validateUTF8(input.content);          // Reject non-UTF-8
    input.content = this.stripNullBytes(input.content);   // Remove \x00
    input.content = this.stripControlChars(input.content); // Keep \n, \r, \t only

    // 2. Content validation (TV-4)
    const trimmed = input.content.trim();
    if (trimmed.length < 20) throw new ValidationError('Signal content too short (min 20 chars)');
    if (trimmed.length > 100_000) throw new ValidationError('Signal content too long (max 100K chars)');

    // 3. Normalize text content
    input.content = this.cleanText(trimmed);   // Collapse whitespace, normalize line endings
    input.content = this.truncateIfNeeded(input.content, MAX_SIGNAL_LENGTH);

    // 4. Source validation
    if (!VALID_SOURCES.includes(input.source)) throw new ValidationError(`Invalid source: ${input.source}`);

    // 5. Timestamp validation (MV-2)
    if (input.timestamp && input.timestamp > Date.now() + 3600000) {
      throw new ValidationError('Signal timestamp is in the future');
    }

    // 6. Source-specific sanitization
    if (input.source === 'web_scrape') {
      input.content = this.sanitizeHTML(input.content);  // Strip <script>, <style>, <iframe>, event handlers (WV-1)
    }

    // 7. Generate ID
    input.id = uuid();

    // 8. Ensure timestamp
    input.timestamp = input.timestamp || new Date();

    // 9. Enrich metadata
    input.metadata.ingested_at = new Date().toISOString();
    input.metadata.content_hash = hash(input.content);    // For dedup
    input.metadata.token_count = countTokens(input.content);
    input.metadata.original_encoding = input.metadata.original_encoding || 'utf-8';

    // 10. Register/update source
    this.sourceRegistry.recordIngestion(input.source, input.source_id);

    return input as RawSignal;
  }
}
```

### 4.1 Per-Adapter Validation Rules

Each adapter enforces source-specific validations **before** passing data to the normalizer:

| Adapter | Validations | Reference |
|---------|-------------|-----------|
| `slack_adapter` | Skip bot messages (SV-1), skip emoji-only (SV-2), thread depth limit 50 (SV-3), file attachment type check (SV-4), dedup by `slack_message_ts` (SV-6) | `08_DATA_CONTRACTS.md` §5.3 |
| `transcript_adapter` | UTF-8 encoding (TV-1), max 500K chars (TV-11), empty content check (TV-4), timestamp format validation | `08_DATA_CONTRACTS.md` §5.2 |
| `document_adapter` | File size ≤50MB (FV-1), file type allowlist (FV-2), MIME/magic bytes match (FV-3), no macros (FV-4), page/row limits (FV-5), no encrypted files (FV-6), corrupt detection (FV-7), empty detection (FV-8) | `08_DATA_CONTRACTS.md` §5.1 |
| `crawler_bot_adapter` | HTML sanitization (WV-1), max 200K chars (WV-2), URL validation (WV-3), 24h URL dedup (WV-4), encoding detection + conversion (WV-5) | `08_DATA_CONTRACTS.md` §5.4 |
| `webhook_receiver` | JSON body ≤10MB (TV-12), JSON depth ≤20 (TV-13), source validation, content-type check (AV-6) | `08_DATA_CONTRACTS.md` §5.2, §5.6 |
| `jira_mcp_adapter` (TBD) | Will validate against JIRA ticket schema, field lengths, project key allowlist | TBD |
| `wiki_mcp_adapter` (TBD) | Will validate page content length, HTML sanitization, attachment handling | TBD |

---

## 5. Ingestion Scheduler Service

Manages recurring ingestion jobs using BullMQ (Redis-backed):

```typescript
// Jobs
const jobs = {
  'slack-ingestion':      { cron: '*/15 * * * *', adapter: 'slack_adapter' },      // Every 15 min
  'consistency-check':    { cron: '0 2 * * *', adapter: 'knowledge_graph_service' }, // Daily 2am
  'embedding-processing': { cron: '*/5 * * * *', adapter: 'embedding_service' },    // Every 5 min
  'trend-analysis':       { cron: '0 6 * * *', adapter: 'trend_analysis_service' }, // Daily 6am
};
```

Manual ingestion (transcripts, documents, crawled content) is triggered on-demand, not scheduled.

---

## 6. Python Document Parser Service

### 6.1 Service Structure

```
python_services/document_parser/
├── app.py                   # FastAPI app
├── parser.py                # Unstructured.io wrapper
├── chunker.py               # Semantic chunking logic
├── transcript_parser.py     # VTT/SRT/speaker-turn parsing
├── requirements.txt         # unstructured[all-docs], fastapi, uvicorn
└── Dockerfile
```

### 6.2 API Contract

```
POST /parse
  Content-Type: multipart/form-data
  Fields: file (binary), format (string)
  Response: {
    "elements": [
      {
        "type": "NarrativeText",
        "text": "...",
        "metadata": { "page_number": 1, "section": "Introduction" }
      }
    ],
    "document_metadata": {
      "pages": 12,
      "word_count": 3456,
      "format": "pptx"
    }
  }

POST /parse-transcript
  Content-Type: application/json
  Body: { "content": "...", "format": "vtt" | "srt" | "text" }
  Response: {
    "segments": [
      {
        "speaker": "Alice",
        "text": "I think we should prioritize the auth fix...",
        "start_time": "00:05:32",
        "end_time": "00:05:48"
      }
    ]
  }

GET /health
  Response: { "status": "ok", "supported_formats": ["pdf", "docx", "pptx", "xlsx", "csv", "txt", "vtt", "srt"] }
```
